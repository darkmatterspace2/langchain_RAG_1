"""
08 - Groq Inference âš¡
======================
Use Groq's ultra-fast inference API with llama-3.3-70b-versatile.
Groq runs LLMs on custom LPU hardware â€” extremely fast responses.

Uses:
  - langchain-groq for the LLM
  - llama-3.3-70b-versatile model
  - Free Groq API key from https://console.groq.com/keys
"""

from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# â”€â”€â”€ Create the Groq LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# llama-3.3-70b supports system prompts!
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.7,
)

# â”€â”€â”€ Example 1: Simple call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 60)
print("âš¡ Groq + LLaMA 3.3 70B â€” Simple Call")
print("=" * 60)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Keep answers concise (2-3 sentences)."),
    ("human", "{question}"),
])

chain = prompt | llm | StrOutputParser()

response = chain.invoke({"question": "What is LangChain and why is it useful?"})
print(f"\n{response}\n")

# â”€â”€â”€ Example 2: Interactive chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 60)
print("ğŸ’¬ Interactive Chat (type 'quit' to exit)")
print("=" * 60)

while True:
    try:
        question = input("\nâ“ You: ").strip()
    except (KeyboardInterrupt, EOFError):
        print("\nğŸ‘‹ Bye!")
        break

    if not question:
        continue
    if question.lower() in ("quit", "exit", "q"):
        print("ğŸ‘‹ Bye!")
        break

    answer = chain.invoke({"question": question})
    print(f"ğŸ¤– LLaMA: {answer}")
